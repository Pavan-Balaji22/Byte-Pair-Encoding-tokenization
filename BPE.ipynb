{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data= open(\"../datasets/llm/shakeshpere.txt\",encoding=\"utf-8\", mode =\"r\").read()\n",
    "len(data)\n",
    "bytesData = [x for x in data.encode(errors=\"replace\")]\n",
    "x =ord(\"a\")\n",
    "len(bytesData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizer:\n",
    "    def __init__(self, nmerges: int):\n",
    "        self.nextByte: int= 256\n",
    "        self.nmerges: int = nmerges\n",
    "        pass\n",
    "    \n",
    "    def _get_counts(self,byteData:list):\n",
    "        counts = {}\n",
    "        for i,j in zip(byteData,byteData[1:]):\n",
    "            counts[i,j]=counts.get((i,j),0)+1\n",
    "        return counts\n",
    "\n",
    "    def _do_merges(self,keys:tuple,token:int,byteData: list) -> list:\n",
    "        i=0\n",
    "        while i < (len(byteData)):\n",
    "            i+=1\n",
    "            if i < (len(byteData)-1) and byteData[i] == keys[0] and byteData[i+1] == keys[1]:\n",
    "                byteData[i] = token\n",
    "                byteData.pop(i+1)\n",
    "                i+=1\n",
    "        return byteData\n",
    "    \n",
    "    def train(self,text:str):\n",
    "        self.bytesData = [x for x in text.encode(errors=\"replace\")]\n",
    "        self.merges_dict: dict = {}\n",
    "        for x in range(self.nmerges):\n",
    "            bp_counts = self._get_counts(self.bytesData)\n",
    "            req_key = max(bp_counts.items(),key=lambda x:x[1])[0]\n",
    "            self.merges_dict[req_key] = self.nextByte + x\n",
    "            self.bytesData = self._do_merges(req_key,self.nextByte+x,self.bytesData)\n",
    "        \n",
    "        self.decode_dict = {v:k for k,v in self.merges_dict.items()}\n",
    "        \n",
    "\n",
    "    def encode(self,text:str) -> list:\n",
    "        tokens = [x for x in text.encode(errors=\"replace\")]\n",
    "        i=0\n",
    "        while i < (len(tokens)):\n",
    "            \n",
    "            if i < (len(tokens)-1) and self.merges_dict.get((tokens[i],tokens[i+1]),False):\n",
    "                tokens[i] = self.merges_dict[(tokens[i],tokens[i+1])]\n",
    "                tokens.pop(i+1)\n",
    "                i+=1\n",
    "            else:\n",
    "                i+=1\n",
    "        \n",
    "        return tokens\n",
    "                \n",
    "\n",
    "    def decode(self,tokens:list) -> str:\n",
    "        bytes = []\n",
    "        i=0\n",
    "        while i < (len(tokens)):\n",
    "            \n",
    "            if self.decode_dict.get(tokens[i],False):\n",
    "                bytes.extend(self.decode_dict[tokens[i]])\n",
    "                i+=1\n",
    "            else:\n",
    "                bytes.extend([tokens[i]])\n",
    "                i+=1\n",
    "        return \"\".join([chr(x) for x in bytes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "merges = vocab_size - 256\n",
    "tokenizer = BPETokenizer(merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "711614"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(data[:1000])) == data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(data[:1000])) == data[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (116, 104): 257,\n",
       " (116, 32): 258,\n",
       " (115, 32): 259,\n",
       " (100, 32): 260,\n",
       " (44, 32): 261,\n",
       " (111, 117): 262,\n",
       " (101, 114): 263,\n",
       " (105, 110): 264,\n",
       " (121, 32): 265,\n",
       " (97, 110): 266,\n",
       " (58, 10): 267,\n",
       " (111, 114): 268,\n",
       " (111, 32): 269,\n",
       " (101, 110): 270,\n",
       " (10, 10): 271,\n",
       " (97, 114): 272,\n",
       " (32, 257): 273,\n",
       " (111, 110): 274,\n",
       " (108, 108): 275,\n",
       " (104, 97): 276,\n",
       " (44, 10): 277,\n",
       " (46, 271): 278,\n",
       " (105, 259): 279,\n",
       " (101, 115): 280,\n",
       " (121, 262): 281,\n",
       " (32, 115): 282,\n",
       " (116, 269): 283,\n",
       " (266, 260): 284,\n",
       " (111, 119): 285,\n",
       " (101, 97): 286,\n",
       " (32, 109): 287,\n",
       " (32, 119): 288,\n",
       " (111, 102): 289,\n",
       " (32, 104): 290,\n",
       " (264, 103): 291,\n",
       " (111, 109): 292,\n",
       " (32, 97): 293,\n",
       " (99, 104): 294,\n",
       " (257, 256): 295,\n",
       " (115, 116): 296,\n",
       " (32, 98): 297,\n",
       " (110, 111): 298,\n",
       " (105, 114): 299,\n",
       " (102, 268): 300,\n",
       " (118, 256): 301,\n",
       " (101, 261): 302,\n",
       " (105, 257): 303,\n",
       " (273, 256): 304,\n",
       " (115, 101): 305,\n",
       " (108, 105): 306,\n",
       " (84, 104): 307,\n",
       " (275, 32): 308,\n",
       " (114, 101): 309,\n",
       " (115, 258): 310,\n",
       " (97, 258): 311,\n",
       " (65, 110): 312,\n",
       " (73, 32): 313,\n",
       " (101, 272): 314,\n",
       " (105, 109): 315,\n",
       " (105, 116): 316,\n",
       " (111, 111): 317,\n",
       " (103, 104): 318,\n",
       " (97, 116): 319,\n",
       " (105, 115): 320,\n",
       " (108, 101): 321,\n",
       " (263, 32): 322,\n",
       " (262, 114): 323,\n",
       " (312, 260): 324,\n",
       " (39, 259): 325,\n",
       " (101, 101): 326,\n",
       " (298, 258): 327,\n",
       " (109, 265): 328,\n",
       " (59, 10): 329,\n",
       " (114, 97): 330,\n",
       " (46, 10): 331,\n",
       " (281, 114): 332,\n",
       " (117, 114): 333,\n",
       " (276, 258): 334,\n",
       " (114, 105): 335,\n",
       " (117, 258): 336,\n",
       " (108, 260): 337,\n",
       " (289, 32): 338,\n",
       " (79, 267): 339,\n",
       " (101, 260): 340,\n",
       " (108, 97): 341,\n",
       " (105, 258): 342,\n",
       " (114, 111): 343,\n",
       " (263, 256): 344,\n",
       " (101, 259): 345,\n",
       " (100, 261): 346,\n",
       " (117, 110): 347,\n",
       " (69, 78): 348,\n",
       " (107, 256): 349,\n",
       " (121, 261): 350,\n",
       " (73, 78): 351,\n",
       " (32, 100): 352,\n",
       " (63, 271): 353,\n",
       " (97, 259): 354,\n",
       " (102, 97): 355,\n",
       " (119, 303): 356,\n",
       " (276, 301): 357,\n",
       " (83, 267): 358,\n",
       " (32, 99): 359,\n",
       " (87, 104): 360,\n",
       " (257, 311): 361,\n",
       " (270, 116): 362,\n",
       " (257, 101): 363,\n",
       " (99, 101): 364,\n",
       " (115, 104): 365,\n",
       " (109, 97): 366,\n",
       " (32, 112): 367,\n",
       " (257, 263): 368,\n",
       " (98, 101): 369,\n",
       " (46, 32): 370,\n",
       " (65, 82): 371,\n",
       " (99, 256): 372,\n",
       " (291, 32): 373,\n",
       " (97, 108): 374,\n",
       " (59, 32): 375,\n",
       " (257, 262): 376,\n",
       " (115, 261): 377,\n",
       " (109, 256): 378,\n",
       " (115, 256): 379,\n",
       " (108, 111): 380,\n",
       " (99, 107): 381,\n",
       " (119, 104): 382,\n",
       " (105, 108): 383,\n",
       " (39, 260): 384,\n",
       " (73, 339): 385,\n",
       " (110, 285): 386,\n",
       " (105, 275): 387,\n",
       " (98, 256): 388,\n",
       " (101, 275): 389,\n",
       " (114, 286): 390,\n",
       " (32, 116): 391,\n",
       " (116, 261): 392,\n",
       " (262, 337): 393,\n",
       " (101, 10): 394,\n",
       " (287, 265): 395,\n",
       " (118, 263): 396,\n",
       " (99, 292): 397,\n",
       " (104, 256): 398,\n",
       " (32, 283): 399,\n",
       " (32, 73): 400,\n",
       " (101, 108): 401,\n",
       " (85, 358): 402,\n",
       " (111, 108): 403,\n",
       " (100, 105): 404,\n",
       " (32, 103): 405,\n",
       " (97, 265): 406,\n",
       " (116, 263): 407,\n",
       " (97, 264): 408,\n",
       " (32, 281): 409,\n",
       " (307, 256): 410,\n",
       " (108, 256): 411,\n",
       " (105, 274): 412,\n",
       " (32, 102): 413,\n",
       " (114, 117): 414,\n",
       " (105, 102): 415,\n",
       " (101, 109): 416,\n",
       " (266, 100): 417,\n",
       " (84, 269): 418,\n",
       " (105, 318): 419,\n",
       " (272, 256): 420,\n",
       " (117, 112): 421,\n",
       " (277, 324): 422,\n",
       " (104, 315): 423,\n",
       " (101, 100): 424,\n",
       " (105, 308): 425,\n",
       " (268, 100): 426,\n",
       " (105, 294): 427,\n",
       " (108, 265): 428,\n",
       " (317, 260): 429,\n",
       " (85, 67): 430,\n",
       " (285, 110): 431,\n",
       " (104, 279): 432,\n",
       " (351, 71): 433,\n",
       " (32, 284): 434,\n",
       " (99, 274): 435,\n",
       " (110, 101): 436,\n",
       " (97, 121): 437,\n",
       " (101, 278): 438,\n",
       " (114, 292): 439,\n",
       " (105, 100): 440,\n",
       " (117, 115): 441,\n",
       " (262, 110): 442,\n",
       " (65, 78): 443,\n",
       " (109, 266): 444,\n",
       " (97, 103): 445,\n",
       " (69, 82): 446,\n",
       " (79, 82): 447,\n",
       " (101, 258): 448,\n",
       " (114, 280): 449,\n",
       " (305, 108): 450,\n",
       " (290, 279): 451,\n",
       " (101, 277): 452,\n",
       " (101, 116): 453,\n",
       " (32, 264): 454,\n",
       " (99, 97): 455,\n",
       " (115, 276): 456,\n",
       " (33, 10): 457,\n",
       " (69, 84): 458,\n",
       " (84, 334): 459,\n",
       " (112, 111): 460,\n",
       " (113, 117): 461,\n",
       " (257, 265): 462,\n",
       " (33, 271): 463,\n",
       " (109, 268): 464,\n",
       " (117, 108): 465,\n",
       " (263, 101): 466,\n",
       " (110, 269): 467,\n",
       " (97, 109): 468,\n",
       " (273, 101): 469,\n",
       " (65, 267): 470,\n",
       " (118, 270): 471,\n",
       " (98, 265): 472,\n",
       " (115, 10): 473,\n",
       " (115, 112): 474,\n",
       " (75, 433): 475,\n",
       " (290, 315): 476,\n",
       " (257, 279): 477,\n",
       " (273, 279): 478,\n",
       " (273, 311): 479,\n",
       " (111, 257): 480,\n",
       " (63, 10): 481,\n",
       " (274, 103): 482,\n",
       " (66, 336): 483,\n",
       " (280, 258): 484,\n",
       " (111, 261): 485,\n",
       " (98, 336): 486,\n",
       " (32, 289): 487,\n",
       " (70, 268): 488,\n",
       " (115, 117): 489,\n",
       " (288, 303): 490,\n",
       " (117, 116): 491,\n",
       " (274, 256): 492,\n",
       " (97, 275): 493,\n",
       " (73, 67): 494,\n",
       " (288, 104): 495,\n",
       " (270, 100): 496,\n",
       " (79, 76): 497,\n",
       " (100, 269): 498,\n",
       " (73, 288): 499}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.merges_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
